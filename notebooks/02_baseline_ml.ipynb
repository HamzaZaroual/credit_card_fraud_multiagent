{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c273ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (227845, 30), y_train shape: (227845,)\n",
      "X_test shape: (56962, 30), y_test shape: (56962,)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9999    0.9756    0.9876     56864\n",
      "           1     0.0609    0.9184    0.1141        98\n",
      "\n",
      "    accuracy                         0.9755     56962\n",
      "   macro avg     0.5304    0.9470    0.5509     56962\n",
      "weighted avg     0.9982    0.9755    0.9861     56962\n",
      "\n",
      "AUPRC: 0.7639\n"
     ]
    }
   ],
   "source": [
    "# 02_baseline_ml.ipynb\n",
    "\n",
    "# ===============================\n",
    "# 1. Import des librairies\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc\n",
    "\n",
    "# ===============================\n",
    "# 2. Chargement des données\n",
    "# ===============================\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv')\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv')\n",
    "\n",
    "# Convertir y_train et y_test en Series\n",
    "y_train = y_train.iloc[:,0]\n",
    "y_test = y_test.iloc[:,0]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Initialisation et entraînement du modèle\n",
    "# ===============================\n",
    "baseline_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# ===============================\n",
    "# 4. Prédiction\n",
    "# ===============================\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "y_pred_proba = baseline_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# ===============================\n",
    "# 5. Évaluation\n",
    "# ===============================\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# AUPRC (Area Under Precision-Recall Curve)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f\"AUPRC: {auprc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ecaef",
   "metadata": {},
   "source": [
    "# Analyse des résultats du modèle Logistic Regression\n",
    "\n",
    "## 1. Taille des jeux de données\n",
    "- **X_train** : `(227845, 30)`  \n",
    "- **X_test** : `(56962, 30)`  \n",
    "\n",
    "> C’est cohérent avec le dataset de Kaggle (28 composantes PCA + `Time` et `Amount` = 30 features).  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Classification report\n",
    "- **Classe 0 (non fraude)** :  \n",
    "  - Precision ≈ 0.9999 → presque toutes les transactions normales sont correctement identifiées.  \n",
    "  - Recall ≈ 0.9756 → environ 2,4% des transactions normales sont mal classées.  \n",
    "\n",
    "- **Classe 1 (fraude)** :  \n",
    "  - Precision ≈ 0.0609 → beaucoup de faux positifs pour la classe fraude.  \n",
    "  - Recall ≈ 0.9184 → le modèle détecte ~92% des fraudes, ce qui est excellent.  \n",
    "\n",
    "- **F1-score** pour la fraude ≈ 0.1141 → très faible à cause de la precision faible.  \n",
    "\n",
    "> Cela montre un déséquilibre typique : le modèle détecte bien la majorité des fraudes (recall élevé) mais produit beaucoup de faux positifs (precision faible).  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Accuracy\n",
    "- Accuracy = 0.9755 → très élevée, mais **trompeuse** sur un dataset déséquilibré.  \n",
    "- Ici, 99,8% des transactions sont normales, donc même un modèle naïf qui prédit toujours “non fraude” aurait ~99% accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. AUPRC\n",
    "- AUPRC ≈ 0.7639 → plutôt bon pour ce dataset déséquilibré.  \n",
    "- L’AUPRC est plus significatif que l’accuracy ici, car il mesure la capacité du modèle à détecter les fraudes tout en limitant les faux positifs.  \n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "**Points forts :**  \n",
    "- Recall très élevé pour la fraude → le modèle détecte la majorité des fraudes.  \n",
    "- AUPRC correct → le modèle est utile pour la détection.  \n",
    "\n",
    "**Points faibles :**  \n",
    "- Precision très faible → beaucoup de faux positifs, ce qui peut être gênant en production (transactions normales bloquées).  \n",
    "\n",
    "---\n",
    "\n",
    "## Améliorations possibles\n",
    "1. **Équilibrer le dataset** : SMOTE, undersampling ou oversampling.  \n",
    "2. **Essayer d’autres modèles** : Random Forest, Gradient Boosting, XGBoost.  \n",
    "3. **Ajuster le threshold** pour réduire les faux positifs.  \n",
    "4. **Modèles coût-sensibles** (le coût d’un faux négatif est élevé).  \n",
    "5. **Ensemble methods** : combiner plusieurs modèles pour améliorer precision et recall simultanément.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
